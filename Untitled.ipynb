{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"random_normal_4:0\", shape=(1152, 10, 16, 8), dtype=float32)\n",
      "Tensor(\"random_normal_5:0\", shape=(32, 1152, 8), dtype=float32)\n",
      "Tensor(\"einsum/transpose_2:0\", shape=(32, 1152, 10, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "W_exp = tf.tile(tf.expand_dims(W, axis=0), [32, 1, 1, 1, 1])\n",
    "X_exp = tf.tile(tf.expand_dims(tf.expand_dims(X, axis=-1), axis=2), [1, 1, 10, 1, 1])\n",
    "\n",
    "Y_matmul = tf.matmul(W_exp, X_exp)\n",
    "\n",
    "print(W_exp, X_exp, Y_matmul, sep='\\n')\n",
    "\n",
    "%%timeit\n",
    "with tf.Session() as sess:\n",
    "    Y.eval()\n",
    "    \n",
    "%%timeit\n",
    "with tf.Session() as sess:\n",
    "    Y_matmul.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load *notMNIST* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train, dev and test: \n",
      "(14979, 28, 28, 1)\n",
      "(1872, 28, 28, 1)\n",
      "(1873, 28, 28, 1)\n",
      "Shape of Y train, dev and test: \n",
      "(14979,)\n",
      "(1872,)\n",
      "(1873,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load data\n",
    "data = np.load('notMNIST.npz')\n",
    "\n",
    "# Get length of data\n",
    "N = len(data['labels'])\n",
    "\n",
    "# Create shuffle index\n",
    "shuffle_idx = np.arange(N)\n",
    "np.random.shuffle(shuffle_idx)\n",
    "\n",
    "\n",
    "# Assign images and labels\n",
    "X = np.expand_dims(data['images'][shuffle_idx] / 255, axis=-1) # Normalise X values\n",
    "Y = data['labels'][shuffle_idx]\n",
    "\n",
    "# Split dataset\n",
    "split_ratio = [0.8, 0.1, 0.1]\n",
    "split_indices = (N * np.cumsum(split_ratio)[:-1]).astype(int)\n",
    "\n",
    "X_train, X_dev, X_test = np.split(X, split_indices)\n",
    "Y_train, Y_dev, Y_test = np.split(Y, split_indices)\n",
    "\n",
    "print('Shape of X train, dev and test: ', X_train.shape, X_dev.shape, X_test.shape, sep='\\n')\n",
    "print('Shape of Y train, dev and test: ', Y_train.shape, Y_dev.shape, Y_test.shape, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "code_folding": [
     38,
     41
    ]
   },
   "outputs": [],
   "source": [
    "def squash(v, axis=-1, name=None):\n",
    "    '''\n",
    "    Nonlinear squashing function for capsule outputs.\n",
    "    \n",
    "    Inputs:\n",
    "        v: A tf tensor\n",
    "        axis: Axis on which to perform squashing\n",
    "        \n",
    "    Returns:\n",
    "        A tensor of the same shape\n",
    "    '''\n",
    "    with tf.name_scope(name, default_name='squashed'):\n",
    "        # For numerical stability\n",
    "        epsilon = 1e-6\n",
    "        safe_norm = tf.sqrt(tf.reduce_sum(v**2, axis=axis, keepdims=True) + epsilon)\n",
    "\n",
    "        squashed = safe_norm**2 / (1 + safe_norm**2) / safe_norm * v\n",
    "\n",
    "        return squashed\n",
    "\n",
    "def routing_by_agreement(input_caps, num_input_caps, num_output_caps, dim_input_caps, dim_output_caps, num_loops=3, name=None):\n",
    "    with tf.variable_scope(name, default_name='routing_by_agreement'):\n",
    "        # Create boolean flag for supposedly faster computations\n",
    "        optimize_comp = False\n",
    "\n",
    "        # Infer batch size\n",
    "        batch_size = tf.shape(input_caps)[0]\n",
    "\n",
    "        # Initialise logits to zero\n",
    "        logits = tf.get_variable('logits', [num_input_caps, num_output_caps], initializer=tf.zeros_initializer())\n",
    "\n",
    "        # Define weights\n",
    "        W = tf.get_variable('W_caps', [num_input_caps, num_output_caps, dim_output_caps, dim_input_caps],\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        # Calculate predictions by lower capsules\n",
    "        preds_by_inputs = tf.einsum('abcd,iad->iabc', W, input_caps, name='preds_by_inputs')\n",
    "\n",
    "        def condition(preds, logits, counter, num_loops, squashed_preds):\n",
    "            return tf.less(counter, num_loops)\n",
    "\n",
    "        def loop(preds, logits, counter, num_loops, squashed_preds):\n",
    "            # Compute routing weights\n",
    "            routing_w = tf.nn.softmax(logits, axis=-1, name='routing_weights')\n",
    "\n",
    "            # Compute mean prediction\n",
    "            mean_preds = tf.einsum('ab,iabc->ibc', routing_w, preds, name='mean_unscaled_pred')\n",
    "\n",
    "            # Squash mean prediction\n",
    "            squashed_preds = squash(mean_preds, axis=-1, name='mean_scaled_pred')\n",
    "\n",
    "            # Calculate agreement and update logits\n",
    "            logits += tf.einsum('iabc,ibc->ab', preds_by_inputs, squashed_preds, name='update_to_logits')\n",
    "\n",
    "            return preds, logits, tf.add(counter, 1), num_loops, squashed_preds\n",
    "\n",
    "        with tf.name_scope('loop'):\n",
    "            counter = tf.constant(0)\n",
    "            output_caps = tf.while_loop(condition, loop, \n",
    "                                        [preds_by_inputs, logits, counter, 3, \n",
    "                                         tf.random_normal([batch_size, num_output_caps, dim_output_caps])])[-1]\n",
    "\n",
    "        return output_caps\n",
    "    \n",
    "def vector_lengths(v, axis=-1, name=None):\n",
    "    '''\n",
    "    Returns the L2-norm of `v` calculated at `axis`.\n",
    "    \n",
    "    Inputs:\n",
    "        v: A tensor of shape (B x n_caps x dim_caps)\n",
    "        \n",
    "    Returns:\n",
    "        A tensor of shape (B x n_caps) containing lengths\n",
    "    '''\n",
    "    with tf.name_scope('lengths'):\n",
    "        # For numerical stability\n",
    "        epsilon = 1e-6\n",
    "\n",
    "        return tf.sqrt(tf.reduce_sum(v**2, axis=axis))\n",
    "    \n",
    "def decoder_network(encodings, images, n_output, n_units=[512, 1024], name=None):\n",
    "    '''\n",
    "    Create a decoder network as regularization.\n",
    "    Network consist of 2 dense layers and one softmax layer.\n",
    "    '''\n",
    "    with tf.name_scope(name, default_name='Decoder'):\n",
    "        # Flatten encodings if not flattened yet\n",
    "        batch_size = tf.shape(encodings)[0]\n",
    "        flattened_enc = tf.reshape(encodings, [batch_size, -1], name='flattened_encs')\n",
    "\n",
    "        # Flatten images\n",
    "        flattened_images = tf.reshape(images, [batch_size, -1], name='flattened_imgs')\n",
    "\n",
    "        # Construct decoder hidden layers\n",
    "        layers = []\n",
    "\n",
    "        for (i, hidden_units) in enumerate(n_units):\n",
    "            layers.append(tf.layers.dense(encodings if i == 0 else layers[-1], \n",
    "                                          hidden_units, \n",
    "                                          activation=tf.nn.relu, \n",
    "                                          name='decoder{}'.format(i)))\n",
    "\n",
    "        # Construct output layer\n",
    "        output = tf.layers.dense(layers[-1], n_output, activation=tf.nn.sigmoid, name='decoder_output')\n",
    "        \n",
    "        # Calculate reconstruction loss\n",
    "        with tf.name_scope('ReconstructionLoss'):\n",
    "            reconstruction_loss = tf.reduce_sum(tf.sqrt((output - flattened_images)**2))\n",
    "\n",
    "        return output, reconstruction_loss\n",
    "    \n",
    "def calc_margin_loss(caps_lengths, labels, m_plus=0.9, m_minus=0.1, lambda_=0.5, name=None):\n",
    "    '''\n",
    "    Calculate the margin loss as per Hinton's paper. \n",
    "    '''\n",
    "    with tf.name_scope(name, default_name='MarginLoss'):\n",
    "        # Create one-hot encoding of labels\n",
    "        target_one_hot = tf.one_hot(tf.cast(labels, tf.int32), depth=tf.shape(caps_lengths)[1], name='target_ohe')\n",
    "\n",
    "        # Calculate margin loss\n",
    "        part1 = target_one_hot * (tf.maximum(tf.constant(0.), m_plus - caps_lengths))**2\n",
    "        part2 = lambda_ * (1 - target_one_hot) * (tf.maximum(tf.constant(0.), caps_lengths - m_minus))**2\n",
    "        \n",
    "        margin_loss = tf.reduce_mean(part1 + part2, name='margin_loss')\n",
    "        \n",
    "        return margin_loss\n",
    "    \n",
    "# def evaluate_tensors(tensors, data, target, batch, batch_size=32, mask_with_labels=True):\n",
    "#     '''\n",
    "#     Wrapper to perform sess.run\n",
    "#     '''\n",
    "#     results = sess.run(tensors, feed_dict={\n",
    "#         _X: data[(batch * batch_size):((batch + 1) * batch_size)],\n",
    "#         _Y: target[(batch * batch_size):((batch + 1) * batch_size)],\n",
    "#         mask_with_labels: mask_with_labels\n",
    "#     })\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholders/Images:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"PrimaryCaps/Conv2/BiasAdd:0\", shape=(?, 6, 6, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "caps_dim = [8, 16]\n",
    "alpha = 0.0005\n",
    "\n",
    "conv1_params = {\n",
    "    'filters': 256,\n",
    "    'kernel_size': 9,\n",
    "    'activation': tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    'filters': 256,\n",
    "    'kernel_size': 9,\n",
    "    'strides': 2,\n",
    "}\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.name_scope('Placeholders'):\n",
    "    _X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='Images')\n",
    "    _Y = tf.placeholder(tf.int64, [None], name='Labels')\n",
    "\n",
    "layer1 = tf.layers.conv2d(_X, name='Conv1', **conv1_params)\n",
    "\n",
    "with tf.name_scope('PrimaryCaps'):\n",
    "    conv2 = tf.layers.conv2d(layer1, name='Conv2', **conv2_params)\n",
    "    batch_size = tf.shape(_X)[0]\n",
    "    print(_X, conv2, sep='\\n')\n",
    "    conv2_reshaped = tf.reshape(conv2, [batch_size, -1, 8], name='conv2_reshaped')\n",
    "    caps1 = squash(conv2_reshaped)\n",
    "    \n",
    "with tf.name_scope('CharCaps'):\n",
    "    caps2 = routing_by_agreement(caps1, num_input_caps=1152, num_output_caps=10, dim_input_caps=8, dim_output_caps=16, name='caps2')\n",
    "    \n",
    "with tf.name_scope('Lengths'):\n",
    "    caps_lengths = vector_lengths(caps2)\n",
    "    \n",
    "with tf.name_scope('Predictions'):\n",
    "    preds = tf.argmax(caps_lengths, axis=-1, name='predictions')\n",
    "    \n",
    "with tf.name_scope('Accuracy'):\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(preds, _Y), tf.float32))\n",
    "      \n",
    "with tf.name_scope('MarginLoss'):\n",
    "    margin_loss = calc_margin_loss(caps_lengths, _Y)\n",
    "\n",
    "with tf.name_scope('ReconstructionLoss'):\n",
    "    # Create Boolean for masking choice\n",
    "    mask_with_labels = tf.placeholder_with_default(False, shape=[], name='mask_with_labels')\n",
    "\n",
    "    # Create reconstruction masks\n",
    "    reconstruction_targets = tf.cond(mask_with_labels, lambda: _Y, lambda: preds, name='reconstruction_targets')\n",
    "    reconstruction_masks = tf.one_hot(reconstruction_targets, depth=10, name='reconstruction_masks')\n",
    "\n",
    "    # Mask encodings\n",
    "    masked_encodings = tf.einsum('ibc,ib->ic', caps2, reconstruction_masks)\n",
    "\n",
    "    # Feed encodings through decoder network\n",
    "    decoder_output, reconstruction_loss = decoder_network(masked_encodings, _X, 784)\n",
    "\n",
    "with tf.name_scope('total_loss'):\n",
    "    total_loss = tf.add(margin_loss, alpha * reconstruction_loss, name='total_loss')\n",
    "\n",
    "    # Add summaries\n",
    "    tf.summary.scalar('margin_loss', margin_loss)\n",
    "    tf.summary.scalar('reconstruction_loss', reconstruction_loss)\n",
    "    tf.summary.scalar('total_loss', total_loss)\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    \n",
    "opt = tf.train.AdamOptimizer().minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "initial_batch_size = 32\n",
    "early_stopping = True\n",
    "\n",
    "early_stop = {'loss': 1e8, 'epoch': 0}\n",
    "\n",
    "NOW = datetime.datetime.now().strftime('%b %d, %Y/%I.%M%p')\n",
    "log_path = 'Logs/{}'.format(NOW)\n",
    "\n",
    "# Create shuffle index\n",
    "train_shuffle_idx = np.arange(len(X_train))\n",
    "dev_shuffle_idx = np.arange(len(X_dev))\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    \n",
    "    # Create summary writers\n",
    "    train_writer = tf.summary.FileWriter('{}/train'.format(log_path), tf.get_default_graph())\n",
    "    dev_writer = tf.summary.FileWriter('{}/dev'.format(log_path))\n",
    "    \n",
    "    # Obtain handle to all summaries\n",
    "    merged = tf.summary.merge_all()\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # Calculate initial summaries\n",
    "    train_summary = sess.run(merged, feed_dict={\n",
    "        _X: X_train[:initial_batch_size],\n",
    "        _Y: Y_train[:initial_batch_size],\n",
    "        mask_with_labels: True\n",
    "    })\n",
    "    train_writer.add_summary(train_summary, epoch + 1)\n",
    "    \n",
    "    dev_summary = sess.run(merged, feed_dict={\n",
    "        _X: X_dev[:initial_batch_size],\n",
    "        _Y: Y_dev[:initial_batch_size],\n",
    "        mask_with_labels: True\n",
    "    })\n",
    "    dev_writer.add_summary(dev_summary, epoch + 1)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Randomise shuffle index\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        \n",
    "        for batch in range(X_train.shape[0] // initial_batch_size):\n",
    "            # Train model\n",
    "            _, loss = sess.run([opt, total_loss], feed_dict={\n",
    "                _X: X_train[train_shuffle_idx][(batch * initial_batch_size):((batch + 1) * initial_batch_size)],\n",
    "                _Y: Y_train[train_shuffle_idx][(batch * initial_batch_size):((batch + 1) * initial_batch_size)],\n",
    "                mask_with_labels: True\n",
    "            })\n",
    "            \n",
    "        # Log summaries and print status updates\n",
    "        if (epoch + 1) % min(50, epochs // 10): # TO CHANGE\n",
    "            if (epoch + 1) % min(1000, epochs // 5):\n",
    "                print('Epoch {} complete'.format(epoch))\n",
    "        \n",
    "            # Add training summary\n",
    "            train_summary = sess.run(merged, feed_dict={\n",
    "                _X: X_train[train_shuffle_idx][:initial_batch_size],\n",
    "                _Y: Y_train[train_shuffle_idx][:initial_batch_size],\n",
    "                mask_with_labels: True\n",
    "            })\n",
    "            train_writer.add_summary(train_summary, epoch + 1)\n",
    "            \n",
    "            # Add validation summary\n",
    "            dev_summary = sess.run(merged, feed_dict={\n",
    "                _X: X_dev[dev_shuffle_idx][:initial_batch_size],\n",
    "                _Y: Y_dev[dev_shuffle_idx][:initial_batch_size],\n",
    "                mask_with_labels: True\n",
    "            })\n",
    "            dev_writer.add_summary(dev_summary, epoch + 1)\n",
    "            \n",
    "            \n",
    "        # Check on validation set\n",
    "        np.random.shuffle(dev_shuffle_idx)\n",
    "        dev_loss = sess.run(total_loss, feed_dict={\n",
    "            _X: X_dev[dev_shuffle_idx][:initial_batch_size],\n",
    "            _Y: Y_dev[dev_shuffle_idx][:initial_batch_size],\n",
    "            mask_with_labels: True\n",
    "        })\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if dev_loss < early_stop['loss']:\n",
    "            early_stop['loss'], early_stop['epoch'] = loss, epoch + 1\n",
    "        elif epoch - early_stop['epoch'] >= epochs // 10:\n",
    "            print('Terminated training due to early stopping.')\n",
    "            break\n",
    "    \n",
    "    # Flush summaries\n",
    "    train_summary.flush()\n",
    "    dev_summary.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
